import asyncio
import random
import logging
import sys
import os
from logging.handlers import TimedRotatingFileHandler
from crawler.wanted_crawler import WantedCrawler
from crawler.saramin_crawler import SaraminCrawler
from crawler.jobkorea_crawler import JobkoreaCrawler
from database.connection import get_session_factory
from repository import RepositoryFactory
from repository.nosql import NoSQLRepository


def setup_logger():
    log_dir = "logs"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    logger = logging.getLogger("IntegratedCrawler")
    logger.setLevel(logging.INFO)
    formatter = logging.Formatter('[%(asctime)s] %(levelname)s | %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    
    stream_handler = logging.StreamHandler(sys.stdout)
    stream_handler.setFormatter(formatter)
    logger.addHandler(stream_handler)
    
    file_handler = TimedRotatingFileHandler(
        filename=f"{log_dir}/crawler.log", 
        when="midnight", 
        interval=1, 
        encoding="utf-8", 
        backupCount=30
    )
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    return logger

logger = setup_logger()

async def run_crawler_task(platform_name, crawler_instance):
    SessionFactory = get_session_factory()
    session = SessionFactory()
    
    try:
        repository = RepositoryFactory.get_repository(platform_name, session)
    except ValueError as e:
        logger.error(f"[{platform_name}] ë¦¬í¬ì§€í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
        session.close()
        return
    
    nosql_repository = NoSQLRepository()

    logger.info(f"ğŸš€ [{platform_name}] í¬ë¡¤ë§ ì‹œì‘")

    total_saved = 0
    pass_page_count = 0 
    limit = 20

    try:
        async with crawler_instance as crawler:
            while True:
                current_page_info = crawler.payload.get('offset', crawler.payload.get('page', 0))
                logger.info(f"[{platform_name}] ğŸ“„ ëª©ë¡ ì¡°íšŒ ì¤‘... (Index: {current_page_info})")

                job_ids = await crawler.fetch_job_list()
                crawler_ids = [str(job_id) for job_id in job_ids]
                
                if not job_ids:
                    logger.info(f"[{platform_name}] âœ… ë” ì´ìƒ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œ.")
                    break

                existing_ids = repository.get_existing_ids(job_ids)
                existing_ids_set = set(str(db_id) for db_id in existing_ids)
                target_ids = [job_id for job_id in crawler_ids if job_id not in existing_ids_set]

                duplicate_count = len(existing_ids)
                target_count = len(target_ids)

                logger.info(f"[{platform_name}] ì¡°íšŒ: {len(job_ids)}ê±´ | íŒ¨ìŠ¤: {duplicate_count}ê±´ | ì‹ ê·œ: {target_count}ê±´")

                if target_count == 0:
                    pass_page_count += 1
                    if pass_page_count >= 5:
                        logger.warning(f"[{platform_name}] â›” ì—°ì† ì¤‘ë³µ ë°œìƒìœ¼ë¡œ ìµœì‹  ê³µê³  ìˆ˜ì§‘ ì™„ë£Œ ê°„ì£¼.")
                        break
                else:
                    pass_page_count = 0

                if target_ids:
                    job_details = await crawler.fetch_details_by_ids(target_ids)
                    
                    logger.info(f"[{platform_name}] ğŸ’¾ DB ì €ì¥ ì¤‘...")
                    for job_data in job_details:
                        repository.save_job(job_data)
                    
                    session.commit()
                    total_saved += len(job_details)
                    logger.info(f"[{platform_name}] âœ… {len(job_details)}ê±´ ì €ì¥ ì™„ë£Œ (ëˆ„ì : {total_saved}ê±´)")

                    nosql_success_count = 0
                    for job_data in job_details:
                        if nosql_repository.save_raw_job(platform_name, job_data):
                            nosql_success_count += 1
                    
                    if nosql_success_count > 0:
                        logger.info(f"[{platform_name}] â˜ï¸ OCI NoSQL {nosql_success_count}/{len(job_details)}ê±´ ì ì¬ ì™„ë£Œ")

                if platform_name == "WANTED":
                    crawler.payload["offset"] += limit
                else:
                    crawler.payload["page"] += 1
                
                sleep_time = random.uniform(3, 7)
                await asyncio.sleep(sleep_time)
                break

    except Exception as e:
        session.rollback()
        logger.error(f"[{platform_name}] ğŸ”¥ ì—ëŸ¬ ë°œìƒ: {e}", exc_info=True)
    finally:
        session.close()
        nosql_repository.close()
        logger.info(f"[{platform_name}] ğŸ ì¢…ë£Œ (ì´ {total_saved}ê±´ ì €ì¥)")


async def main():
    logger.info("============== [í†µí•© í¬ë¡¤ëŸ¬ ì‹œì‘] ==============")
    
    await asyncio.gather(
        run_crawler_task("WANTED", WantedCrawler()),
        run_crawler_task("SARAMIN", SaraminCrawler()),
        run_crawler_task("JOBKOREA", JobkoreaCrawler())
    )
    
    logger.info("============== [ëª¨ë“  í¬ë¡¤ëŸ¬ ì¢…ë£Œ] ==============")

if __name__ == "__main__":
    asyncio.run(main())