import asyncio
import random
import logging
import sys
import os
from utils.logger import setup_logger
from crawler.wanted_crawler import WantedCrawler
from crawler.saramin_crawler import SaraminCrawler
from crawler.jobkorea_crawler import JobkoreaCrawler
from database.connection import get_session_factory
from repository import RepositoryFactory
from repository.nosql import NoSQLRepository


async def run_crawler_task(platform_name, crawler_instance, logger, nosql_repository):
    logger = logger.getChild(platform_name)
    SessionFactory = get_session_factory()
    session = SessionFactory()
    
    try:
        child_logger = logger.getChild("Repository")
        repository = RepositoryFactory.get_repository(platform_name, session, child_logger)
    except ValueError as e:
        logger.error(f"ë¦¬í¬ì§€í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
        session.close()
        return

    logger.info(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘")

    total_saved = 0
    pass_page_count = 0 
    limit = 20

    try:
        async with crawler_instance as crawler:
            while True:
                current_page_info = crawler.payload.get('offset', crawler.payload.get('page', 0))
                logger.info(f"ğŸ“„ ëª©ë¡ ì¡°íšŒ ì¤‘... (Index: {current_page_info})")

                job_ids = await crawler.fetch_job_list()
                crawler_ids = [str(job_id) for job_id in job_ids]
                
                if not job_ids:
                    logger.info(f"âœ… ë” ì´ìƒ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œ.")
                    break

                existing_ids = repository.get_existing_ids(job_ids)
                existing_ids_set = set(str(db_id) for db_id in existing_ids)
                target_ids = [job_id for job_id in crawler_ids if job_id not in existing_ids_set]

                duplicate_count = len(existing_ids)
                target_count = len(target_ids)

                logger.info(f"ì¡°íšŒ: {len(job_ids)}ê±´ | íŒ¨ìŠ¤: {duplicate_count}ê±´ | ì‹ ê·œ: {target_count}ê±´")

                if target_count == 0:
                    pass_page_count += 1
                    if pass_page_count >= 5:
                        logger.warning(f"â›” ì—°ì† ì¤‘ë³µ ë°œìƒìœ¼ë¡œ ìµœì‹  ê³µê³  ìˆ˜ì§‘ ì™„ë£Œ ê°„ì£¼.")
                        break
                else:
                    pass_page_count = 0

                if target_ids:
                    job_details = await crawler.fetch_details_by_ids(target_ids)
                    
                    logger.info(f"ğŸ’¾ DB ì €ì¥ ì¤‘...")
                    for job_data in job_details:
                        repository.save_job(job_data)
                    
                    session.commit()
                    total_saved += len(job_details)
                    logger.info(f"âœ… {len(job_details)}ê±´ ì €ì¥ ì™„ë£Œ (ëˆ„ì : {total_saved}ê±´)")

                    nosql_success_count = 0
                    for job_data in job_details:
                        if nosql_repository.save_raw_job(platform_name, job_data.get("job")):
                            nosql_success_count += 1
                    
                    if nosql_success_count > 0:
                        logger.info(f"â˜ï¸ OCI NoSQL {nosql_success_count}/{len(job_details)}ê±´ ì ì¬ ì™„ë£Œ")

                if platform_name == "WANTED":
                    crawler.payload["offset"] += limit
                else:
                    crawler.payload["page"] += 1
                
                sleep_time = random.uniform(3, 7)
                await asyncio.sleep(sleep_time)

    except Exception as e:
        session.rollback()
        logger.error(f"ğŸ”¥ ì—ëŸ¬ ë°œìƒ: {e}", exc_info=True)
    finally:
        session.close()
        logger.info(f"ğŸ ì¢…ë£Œ (ì´ {total_saved}ê±´ ì €ì¥)")


async def main():
    logger = setup_logger("Crawler")
    logger.info("============== [í†µí•© í¬ë¡¤ëŸ¬ ì‹œì‘] ==============")

    nosql_repository = NoSQLRepository(logger)
    try:
        await asyncio.gather(
            run_crawler_task("WANTED", WantedCrawler(logger=logger), logger, nosql_repository),
            run_crawler_task("SARAMIN", SaraminCrawler(logger=logger), logger, nosql_repository),
            run_crawler_task("JOBKOREA", JobkoreaCrawler(logger=logger), logger, nosql_repository)
        )
    finally:
        nosql_repository.close()
        logger.info("============== [ëª¨ë“  í¬ë¡¤ëŸ¬ ì¢…ë£Œ] ==============")

if __name__ == "__main__":
    asyncio.run(main())