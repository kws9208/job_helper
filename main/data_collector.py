import asyncio
import random
import logging
import sys
import os
from utils.logger import setup_logger
from crawler.wanted_crawler import WantedCrawler
from crawler.saramin_crawler import SaraminCrawler
from crawler.jobkorea_crawler import JobkoreaCrawler
from database.connection import get_session_factory
from repository import RepositoryFactory
from repository.nosql import NoSQLRepository
from collections import Counter


async def run_crawler_task(platform_name, crawler_instance, logger, nosql_repository, max_page_count=100):
    logger = logger.getChild(platform_name)
    SessionFactory = get_session_factory()
    session = SessionFactory()
    
    try:
        child_logger = logger.getChild("Repository")
        repository = RepositoryFactory.get_repository(platform_name, session, child_logger)
    except ValueError as e:
        logger.error(f"ë¦¬í¬ì§€í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
        session.close()
        return

    logger.info(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘")

    total_saved = 0
    pass_page_count = 0 
    limit = 20

    try:
        async with crawler_instance as crawler:
            while True:
                current_page_info = crawler.payload.get('offset', crawler.payload.get('page', 0))
                logger.info(f"ğŸ“„ ëª©ë¡ ì¡°íšŒ ì¤‘... (Index: {current_page_info})")

                job_ids = await crawler.fetch_job_list()
                crawler_ids = [str(job_id) for job_id in job_ids]
                
                if not job_ids:
                    logger.info(f"âœ… ë” ì´ìƒ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œ.")
                    break

                need_crawling_flags = [repository.need_job_crawling(job_id, expire_days=7) for job_id in job_ids]
                
                target_ids = [job_id for job_id, flag in zip(job_ids, need_crawling_flags) if flag in ("new", "renew")]
                target_count = len(target_ids)

                counter = Counter(need_crawling_flags)
                logger.info(f"ì¡°íšŒ: {len(job_ids)}ê±´ | ì‹ ê·œ: {counter["new"]}ê±´ | íŒ¨ìŠ¤: {counter["pass"]}ê±´ | ê°±ì‹ : {counter["renew"]}")

                if target_count == 0:
                    pass_page_count += 1
                    if pass_page_count >= max_page_count:
                        logger.warning(f"â›” {max_page_count} í˜ì´ì§€ ì´ìƒ ì—°ì† ì¤‘ë³µ ë°œìƒìœ¼ë¡œ ìµœì‹  ê³µê³  ìˆ˜ì§‘ ì™„ë£Œ ê°„ì£¼.")
                        break
                else:
                    pass_page_count = 0

                if target_ids:
                    tasks = [process_single_job(platform_name, crawler, repository, target_id) for target_id in target_ids]
                    results = await asyncio.gather(*tasks)
                    job_details = [res for res in results if res is not None]

                    logger.info(f"ğŸ’¾ DB ì €ì¥ ì¤‘...")
                    for job_data in job_details:
                        repository.save_job(job_data)
                    
                    session.commit()
                    total_saved += len(job_details)
                    logger.info(f"âœ… {len(job_details)}ê±´ ì €ì¥ ì™„ë£Œ (ëˆ„ì : {total_saved}ê±´)")

                    nosql_success_count = 0
                    for job_data in job_details:
                        if nosql_repository.save_raw_job(platform_name, job_data.get("job")):
                            nosql_success_count += 1
                    
                    if nosql_success_count > 0:
                        logger.info(f"â˜ï¸ OCI NoSQL {nosql_success_count}/{len(job_details)}ê±´ ì ì¬ ì™„ë£Œ")

                if platform_name == "WANTED":
                    crawler.payload["offset"] += limit
                else:
                    crawler.payload["page"] += 1
                
                sleep_time = random.uniform(3, 7)
                await asyncio.sleep(sleep_time)

    except Exception as e:
        session.rollback()
        logger.error(f"ğŸ”¥ ì—ëŸ¬ ë°œìƒ: {e}", exc_info=True)
    finally:
        session.close()
        logger.info(f"ğŸ ì¢…ë£Œ (ì´ {total_saved}ê±´ ì €ì¥)")

async def process_single_job(platform_name, crawler, repository, target_id):
    if platform_name == "JOBKOREA":
        job_summaray = await crawler.fetch_job_summary(target_id)
        if job_summaray is None:
            return
        detail_contents = await crawler.fetch_job_detail(target_id)
        company_id = job_summaray.get("company_id")
        if job_summaray["company_info"] is not None and repository.need_company_crawling(company_id, expire_days=7) in ("new", "renew"):
            if company_url := job_summaray["company_info"]["company_url"]:
                company_info = await crawler.fetch_company_info(company_url)
            else:
                company_info = dict()
            company_info = company_info | job_summaray.pop("company_info")
        else:
            del job_summaray["company_info"]
            company_info = None

        data = {
            "company": company_info,
            "job": {
                **job_summaray, 
                **detail_contents
            }
        }

    elif platform_name == "SARAMIN":
        job_summaray = await crawler.fetch_job_summary(target_id)
        if job_summaray is None:
            return
        detail_contents = await crawler.fetch_job_detail(target_id)
        csn = job_summaray.get("csn")
        if job_summaray["company_info"] is not None and  repository.need_company_crawling(csn, expire_days=7) in ("new", "renew"):
            if company_url := job_summaray["company_info"]["company_url"]:
                company_info = await crawler.fetch_company_info(company_url)
            else:
                company_info = dict()
            company_info = company_info | job_summaray.pop("company_info")
        else:
            del job_summaray["company_info"]
            company_info = None

        data = {
            "company": company_info,
            "job": {
                **job_summaray, 
                **detail_contents
            }
        }

    elif platform_name == "WANTED":
        job_detail_data = await crawler.fetch_job_detail(target_id)
        company_id = job_detail_data.get("company_id")
        if repository.need_company_crawling(company_id, expire_days=7) in ("new", "renew"):
            if company_id := job_detail_data.get('company_id'):
                company_info_data = await crawler.fetch_company_info(company_id)
        else:
            company_info_data = None

        data = {
            "company": company_info_data if company_id else None,
            "job": job_detail_data,
        }

    return data

async def main():
    logger = setup_logger("Crawler")
    logger.info("============== [í†µí•© í¬ë¡¤ëŸ¬ ì‹œì‘] ==============")

    nosql_repository = NoSQLRepository(logger)
    try:
        await asyncio.gather(
            run_crawler_task("WANTED", WantedCrawler(logger=logger), logger, nosql_repository),
            run_crawler_task("SARAMIN", SaraminCrawler(logger=logger), logger, nosql_repository),
            run_crawler_task("JOBKOREA", JobkoreaCrawler(logger=logger), logger, nosql_repository)
        )
    finally:
        nosql_repository.close()
        logger.info("============== [ëª¨ë“  í¬ë¡¤ëŸ¬ ì¢…ë£Œ] ==============")

if __name__ == "__main__":
    asyncio.run(main())