from .base_crawler import BaseCrawler
import time
import asyncio
import httpx
import json
import traceback
import pprint

class WantedCrawler(BaseCrawler):
    def __init__(self, k=5):
        super().__init__(base_url="https://www.wanted.co.kr", platform="Wanted", k=k)
        self.job_list_url = self.base_url + "/api/chaos/navigation/v1/results"
        self.job_detail_url = self.base_url + "/api/chaos/jobs/v4"
        self.payload = {
            str(int(time.time() * 1000)): "",
            "country": "kr",
            "job_sort": "job.latest_order",
            "years": "-1",
            "locations": "all",
            "limit": 20,
            "offset": 0
        }

    async def fetch_job_list(self):
        job_list_response = await self.request('GET', self.job_list_url, headers=self.header, params=self.payload)
        job_ids = [job['id'] for job in job_list_response.json()["data"]]
        return job_ids

    async def fetch_details_by_ids(self, job_ids):
        print(f" {self.platform} | ğŸ” {len(job_ids)}ê°œì˜ ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘ ì‹œì‘...")
        tasks = [self.fetch_job_detail(job_id) for job_id in job_ids]
        results = await asyncio.gather(*tasks)
        return [job for job in results if job is not None]
    
    async def fetch_job_detail(self, job_id):
        job_url = f"{self.job_detail_url}/{job_id}/details"
        try:
            job_detail_response = await self.request('GET', job_url)
        except Exception:
            return None
        return self.parse_job_data(job_detail_response.json(), job_url)

    def parse_job_data(self, details_json, url):
        try:
            if details_json.get('error') is None and details_json.get("message") == "ok":
                job_data = details_json['data']['job']
                job_details_dict = job_data.get('detail', {})

                return {
                    "job_id": job_data.get('id'),
                    "job_url": url,
                    "position": job_details_dict.get('position'),
                    "is_active": True if job_data.get('status') == "active" else False,
                    "deadline": job_data.get('due_time') if job_data.get('due_time') else "ìƒì‹œì±„ìš©",
                    "detail": job_details_dict,
                    "attraction_tags": [tag['title'] for tag in job_data.get('attraction_tags', [])],
                    "company_id": job_data.get('company', {}).get('id'),
                    "company_name": job_data.get('company', {}).get('name'),
                    "company_logo": job_data.get('company', {}).get('logo_img').get('origin'),
                    "address": job_data.get('address', {}).get('full_location'),
                    "category_tag": job_data.get('category_tag', {}).get('parent_tag').get('text'),
                    "detail_tags": [child_tag['text'] for child_tag in job_data.get('category_tag', {}).get('child_tags', [])],
                    "skill_tags": [skill['text'] for skill in job_data.get('skill_tags', [])],
                    "annual_from": job_data.get('annual_from'),
                    "annual_to": job_data.get('annual_to'),
                    "employment_type": job_data.get('employment_type')
                }
            else:
                print(f"[API ì—ëŸ¬] {details_json.get('message')}")
                return None
        except Exception as e:
            print(f"[íŒŒì‹± ì—ëŸ¬] id: {job_data.get('id')} / {e}")
            traceback.print_exc()
            return None

    async def run(self):
        print("=== Wanted í¬ë¡¤ëŸ¬ ì‹œì‘ ===")
        job_ids = await self.fetch_job_list()

        if not job_ids:
            print("ìˆ˜ì§‘ëœ ê³µê³  IDê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"ì´ {len(job_ids)}ê°œì˜ ê³µê³ ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")

        tasks = [self.fetch_job_detail(job_id) for job_id in job_ids]
        
        results = await asyncio.gather(*tasks)

        final_jobs = [job for job in results if job is not None]
        print(f"=== Wanted í¬ë¡¤ëŸ¬ ì¢…ë£Œ (ì„±ê³µ: {len(final_jobs)}ê±´) ===")
        return final_jobs

async def main():
    print("ğŸš€ [í…ŒìŠ¤íŠ¸] ì›í‹°ë“œ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘...")
    async with WantedCrawler() as crawler:
        start = time.time()
        results = await crawler.run()
        end = time.time()
        
        if results:
            print(f"\nâœ… ì´ {len(results)}ê°œì˜ ê³µê³  ìˆ˜ì§‘ ì™„ë£Œ!")
            print("--- [ì²« ë²ˆì§¸ ê³µê³  ìƒ˜í”Œ ë°ì´í„°] ---")
            pprint.pprint(results[0])
            print([res['deadline'] for res in results])
        else:
            print("\nâŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (API ì‘ë‹µ í™•ì¸ í•„ìš”)")
        print("ì†Œìš”ì‹œê°„:", end - start)

if __name__ == "__main__":
    asyncio.run(main())